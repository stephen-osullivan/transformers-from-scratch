{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial shape torch.Size([2, 3])\n",
      "tensor([[66, 85, 31],\n",
      "        [14, 74, 53]])\n",
      "\n",
      "Shape after embedding torch.Size([2, 3, 4])\n",
      "tensor([[[ 0.3844,  0.0395, -0.7434,  1.5816],\n",
      "         [-0.3342,  0.8615,  0.4445, -1.3960],\n",
      "         [ 2.1648,  0.4029, -0.2743,  1.3082]],\n",
      "\n",
      "        [[-0.2259, -0.1743, -0.4262, -1.4836],\n",
      "         [-0.9625, -0.6392,  0.5664, -0.5657],\n",
      "         [-0.4407,  0.7306, -1.9145,  0.2075]]], grad_fn=<EmbeddingBackward0>)\n",
      "\n",
      "Q shape torch.Size([2, 3, 5])\n",
      "K shape torch.Size([2, 3, 5])\n",
      "V shape torch.Size([2, 3, 5])\n",
      "\n",
      "Scores shape torch.Size([2, 3, 3])\n",
      "tensor([[[ 0.0290, -0.3966,  0.1607],\n",
      "         [-0.2134,  0.2727, -0.6381],\n",
      "         [-0.1263, -0.2367, -0.4838]],\n",
      "\n",
      "        [[-0.0196,  0.2575, -0.0710],\n",
      "         [-0.1938, -0.1350, -0.2213],\n",
      "         [-0.1482, -0.1566,  0.1393]]], grad_fn=<DivBackward0>)\n",
      "\n",
      "Causal scores shape torch.Size([2, 3, 3])\n",
      "tensor([[[ 0.0290,    -inf,    -inf],\n",
      "         [-0.2134,  0.2727,    -inf],\n",
      "         [-0.1263, -0.2367, -0.4838]],\n",
      "\n",
      "        [[-0.0196,    -inf,    -inf],\n",
      "         [-0.1938, -0.1350,    -inf],\n",
      "         [-0.1482, -0.1566,  0.1393]]], grad_fn=<MaskedFillBackward0>)\n",
      "\n",
      "Weights shape torch.Size([2, 3, 3])\n",
      "tensor([[[1.0000, 0.0000, 0.0000],\n",
      "         [0.3808, 0.6192, 0.0000],\n",
      "         [0.3854, 0.3451, 0.2695]],\n",
      "\n",
      "        [[1.0000, 0.0000, 0.0000],\n",
      "         [0.4853, 0.5147, 0.0000],\n",
      "         [0.3008, 0.2983, 0.4010]]], grad_fn=<SoftmaxBackward0>)\n",
      "\n",
      "weighted values shape torch.Size([2, 3, 5])\n",
      "tensor([[[ 0.4180, -0.3835,  0.2769, -0.4581, -0.3380],\n",
      "         [ 0.1939, -0.3540, -0.1278,  0.0844, -0.3686],\n",
      "         [ 0.4006, -0.5250,  0.2290, -0.2987, -0.4411]],\n",
      "\n",
      "        [[-0.2337,  0.2408, -0.9002,  0.1293, -0.1252],\n",
      "         [-0.4272,  0.1094, -0.8357, -0.0018, -0.2787],\n",
      "         [-0.0326,  0.1834, -0.6703,  0.2250, -0.0739]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "\n",
      "Final Output shape torch.Size([2, 3, 5])\n",
      "tensor([[[-0.7386,  0.4658,  0.3282, -0.3058,  0.3390],\n",
      "         [-0.4270,  0.1481,  0.2950, -0.4126,  0.1076],\n",
      "         [-0.7072,  0.3183,  0.3605, -0.3840,  0.2842]],\n",
      "\n",
      "        [[-0.0308,  0.2154,  0.1117, -0.2438, -0.1431],\n",
      "         [-0.1495,  0.1656,  0.2038, -0.3354, -0.1664],\n",
      "         [-0.0721,  0.2295,  0.1125, -0.2491, -0.0517]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# single head attention\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 2\n",
    "seq_len = 3\n",
    "embedding_dim = 4\n",
    "head_size = 5\n",
    "\n",
    "x = torch.randint(0, 100, (batch_size, seq_len)) # (batch_size, seq_len)\n",
    "\n",
    "print('Initial shape', x.shape)\n",
    "print(x)\n",
    "print()\n",
    "\n",
    "#embdding\n",
    "embedding_layer = nn.Embedding(100, embedding_dim) # (vocab_size, input_dim)\n",
    "x = embedding_layer(x) # (batch_size, seq_len, embedding_dim)\n",
    "print('Shape after embedding', x.shape)\n",
    "print(x)\n",
    "print()\n",
    "\n",
    "# Linear layers for Q, K, V projections\n",
    "W_q = nn.Linear(embedding_dim, head_size)\n",
    "W_k = nn.Linear(embedding_dim, head_size)\n",
    "W_v = nn.Linear(embedding_dim, head_size)\n",
    "W_o = nn.Linear(head_size, head_size)\n",
    "\n",
    "### FORWARD PASS\n",
    "# Assuming x is of shape (batch_size, seq_len, input_dim)\n",
    "# Project inputs to Q, K, V\n",
    "Q = W_q(x) # (batch_size, seq_len, head_size)\n",
    "K = W_k(x) # (batch_size, seq_len, head_size)\n",
    "V = W_v(x) # (batch_size, seq_len, head_size)\n",
    "print('Q shape', Q.shape)\n",
    "print('K shape', K.shape)\n",
    "print('V shape', V.shape)\n",
    "print()\n",
    "\n",
    "# Compute attention scores\n",
    "scores = torch.matmul(Q, K.transpose(-2, -1)) / (K.size(-1) ** 0.5) # (batch_size, seq_len, seq_len)\n",
    "print('Scores shape', scores.shape)\n",
    "print(scores)\n",
    "print()\n",
    "\n",
    "# apply causal mask\n",
    "causal_mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool()\n",
    "scores = scores.masked_fill(causal_mask, float('-inf'))\n",
    "print('Causal scores shape', scores.shape)\n",
    "print(scores)\n",
    "print()\n",
    "weights = torch.softmax(scores, dim=-1) # (batch_size, seq_len, seq_len)\n",
    "print('Weights shape', weights.shape)\n",
    "print(weights)\n",
    "print()\n",
    "# Apply attention weights to V\n",
    "output = torch.matmul(weights, V) # (batch_size, seq_len, head_size)\n",
    "print('weighted values shape', output.shape)\n",
    "print(output)\n",
    "print()\n",
    "# Project back to original dimension\n",
    "output = W_o(output)\n",
    "print('Final Output shape', output.shape)\n",
    "print(output)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial shape torch.Size([2, 3])\n",
      "\n",
      "Shape after embedding torch.Size([2, 3, 8])\n",
      "\n",
      "Q shape torch.Size([2, 3, 8])\n",
      "K shape torch.Size([2, 3, 8])\n",
      "V shape torch.Size([2, 3, 8])\n",
      "\n",
      "Scores shape torch.Size([2, 4, 3, 3])\n",
      "\n",
      "Causal scores shape torch.Size([2, 4, 3, 3])\n",
      "\n",
      "Weights shape torch.Size([2, 4, 3, 3])\n",
      "tensor([[[[1.0000, 0.0000, 0.0000],\n",
      "          [0.4477, 0.5523, 0.0000],\n",
      "          [0.3448, 0.3598, 0.2954]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [0.4888, 0.5112, 0.0000],\n",
      "          [0.2664, 0.4853, 0.2483]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [0.5272, 0.4728, 0.0000],\n",
      "          [0.2904, 0.3663, 0.3433]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [0.3566, 0.6434, 0.0000],\n",
      "          [0.2164, 0.3818, 0.4018]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000],\n",
      "          [0.4978, 0.5022, 0.0000],\n",
      "          [0.3588, 0.2964, 0.3448]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [0.5675, 0.4325, 0.0000],\n",
      "          [0.3245, 0.3506, 0.3249]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [0.5583, 0.4417, 0.0000],\n",
      "          [0.3160, 0.3521, 0.3319]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [0.4599, 0.5401, 0.0000],\n",
      "          [0.3101, 0.3390, 0.3508]]]], grad_fn=<SoftmaxBackward0>)\n",
      "\n",
      "weighted values shape torch.Size([2, 4, 3, 2])\n",
      "\n",
      "Final Output shape torch.Size([2, 3, 8])\n"
     ]
    }
   ],
   "source": [
    " # multi head attention\n",
    "\n",
    "# single head attention\n",
    "from torch import nn\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 2\n",
    "seq_len = 3\n",
    "embedding_dim = 8\n",
    "num_heads = 4\n",
    "head_size = embedding_dim // num_heads\n",
    "\n",
    "x = torch.randint(0, 100, (batch_size, seq_len)) # (batch_size, seq_len)\n",
    "\n",
    "print('Initial shape', x.shape)\n",
    "print()\n",
    "\n",
    "#embdding\n",
    "embedding_layer = nn.Embedding(100, embedding_dim) # (vocab_size, input_dim)\n",
    "x = embedding_layer(x) # (batch_size, seq_len, embedding_dim)\n",
    "print('Shape after embedding', x.shape)\n",
    "print()\n",
    "\n",
    "# Linear layers for Q, K, V projections\n",
    "W_q = nn.Linear(embedding_dim, embedding_dim)\n",
    "W_k = nn.Linear(embedding_dim, embedding_dim)\n",
    "W_v = nn.Linear(embedding_dim, embedding_dim)\n",
    "W_o = nn.Linear(embedding_dim, embedding_dim)\n",
    "\n",
    "### FORWARD PASS\n",
    "# Assuming x is of shape (batch_size, seq_len, input_dim)\n",
    "# Project inputs to Q, K, V\n",
    "Q = W_q(x) # (batch_size, seq_len, embedding_dim)\n",
    "K = W_k(x) # (batch_size, seq_len, embedding_dim)\n",
    "V = W_v(x) # (batch_size, seq_len, embedding_dim)\n",
    "print('Q shape', Q.shape)\n",
    "print('K shape', K.shape)\n",
    "print('V shape', V.shape)\n",
    "print()\n",
    "\n",
    "# split heads\n",
    "Q = Q.view(batch_size, seq_len, num_heads, head_size).transpose(1, 2) # (batch_size, num_heads, seq_len, head_size)\n",
    "K = K.view(batch_size, seq_len, num_heads, head_size).transpose(1, 2) # (batch_size, num_heads, seq_len, head_size)      \n",
    "V = V.view(batch_size, seq_len, num_heads, head_size).transpose(1, 2) # (batch_size, num_heads, seq_len, head_size)\n",
    "\n",
    "# Compute attention scores\n",
    "scores = torch.matmul(Q, K.transpose(-2, -1)) / (K.size(-1) ** 0.5) # (batch_size, num_heads, seq_len, seq_len)\n",
    "print('Scores shape', scores.shape)\n",
    "print()\n",
    "\n",
    "# apply causal mask\n",
    "causal_mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool()\n",
    "scores = scores.masked_fill(causal_mask, float('-inf'))\n",
    "print('Causal scores shape', scores.shape)\n",
    "print()\n",
    "\n",
    "weights = torch.softmax(scores, dim=-1) # (batch_size, num_heads, seq_len, seq_len)\n",
    "print('Weights shape', weights.shape)\n",
    "print(weights)\n",
    "print()\n",
    "\n",
    "# Apply attention weights to V\n",
    "output = torch.matmul(weights, V) # batch_size, num_heads, seq_len, head_size)\n",
    "print('weighted values shape', output.shape)\n",
    "print()\n",
    "\n",
    "# Project back to original dimension\n",
    "# we need to use continguous before applying view as the output of transpose is non-continguous in memory\n",
    "output = W_o(output.transpose(1, 2).contiguous().view(batch_size, seq_len, embedding_dim))\n",
    "print('Final Output shape', output.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Pytorch's attention layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.4867, -0.6963,  0.3553,  1.6682, -0.8163,  0.4333, -0.7774,\n",
       "           -0.2174],\n",
       "          [ 0.4151, -0.8389,  0.5260,  1.5996, -0.6560,  0.3786, -0.8686,\n",
       "           -0.2580],\n",
       "          [ 0.2302, -0.4075,  0.3258,  1.1119, -0.4806,  0.3360, -0.5904,\n",
       "           -0.1410]],\n",
       " \n",
       "         [[-0.7617,  0.1197,  0.2304, -1.1195,  0.5100, -1.3840,  0.8277,\n",
       "           -0.8969],\n",
       "          [-0.4709,  0.5864, -0.3686, -0.8999,  0.2155, -0.6151,  0.6710,\n",
       "           -0.2450],\n",
       "          [-0.0963, -0.0068,  0.0778, -0.2031, -0.1081, -0.1642, -0.1416,\n",
       "           -0.1137]]], grad_fn=<TransposeBackward0>),\n",
       " tensor([[[1.0000, 0.0000, 0.0000],\n",
       "          [0.4459, 0.5541, 0.0000],\n",
       "          [0.3307, 0.4116, 0.2577]],\n",
       " \n",
       "         [[1.0000, 0.0000, 0.0000],\n",
       "          [0.4688, 0.5312, 0.0000],\n",
       "          [0.3090, 0.3182, 0.3728]]], grad_fn=<MeanBackward1>))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 2\n",
    "seq_len = 3\n",
    "embedding_dim = 8\n",
    "num_heads = 4\n",
    "head_size = embedding_dim // num_heads\n",
    "\n",
    "x = torch.randint(0, 100, (batch_size, seq_len)) # (batch_size, seq_len)\n",
    "\n",
    "#embdding\n",
    "embedding_layer = nn.Embedding(100, embedding_dim) # (vocab_size, input_dim)\n",
    "x = embedding_layer(x) # (batch_size, seq_len, embedding_dim)\n",
    "\n",
    "attention = nn.MultiheadAttention(\n",
    "    embed_dim=embedding_dim,\n",
    "    num_heads=num_heads,\n",
    "    batch_first=True  # Use (batch, seq, features) format\n",
    ")\n",
    "\n",
    "\n",
    "causal_mask = torch.triu(\n",
    "    torch.ones(seq_len, seq_len, dtype=bool), \n",
    "    diagonal=1\n",
    ")\n",
    "\n",
    "attention(x, x, x, is_causal=True, attn_mask=causal_mask) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[-0.3527, -0.4131,  0.2531, -0.1708, -0.2861, -0.1642,  0.2186, -0.2150],\n",
       "         [-0.3680,  0.3517,  0.2795,  0.1738, -0.1350, -0.1377,  0.1143,  0.1266],\n",
       "         [ 0.4017, -0.1394,  0.1766,  0.1344, -0.1631, -0.0773, -0.4112,  0.2540],\n",
       "         [ 0.1289,  0.3852, -0.1793, -0.1650, -0.0402, -0.3146,  0.3904, -0.3643],\n",
       "         [-0.1100,  0.0730,  0.4076,  0.0636, -0.2499,  0.0209, -0.1799,  0.1883],\n",
       "         [-0.3686, -0.3249, -0.3289,  0.0247, -0.1940, -0.3537, -0.1217, -0.3896],\n",
       "         [-0.1471,  0.2450, -0.3825,  0.1600, -0.0596,  0.4094, -0.1570, -0.0447],\n",
       "         [ 0.1567,  0.0915, -0.1380,  0.0324, -0.4106,  0.4170, -0.3286,  0.0297],\n",
       "         [ 0.1417,  0.2760,  0.0196,  0.0301, -0.3777, -0.2168, -0.1833,  0.1492],\n",
       "         [ 0.2148,  0.1869, -0.3434,  0.0595,  0.0860,  0.1215,  0.1664,  0.0398],\n",
       "         [ 0.1115,  0.3742,  0.2065,  0.2267, -0.1438,  0.1401, -0.2448,  0.2265],\n",
       "         [-0.1933, -0.2717, -0.0476,  0.3238, -0.3507,  0.0037,  0.0700,  0.4280],\n",
       "         [ 0.0219,  0.2695,  0.0573, -0.0859,  0.0764,  0.2195,  0.0316,  0.3886],\n",
       "         [-0.1830,  0.3978,  0.3447,  0.0195,  0.0544, -0.2605,  0.0043, -0.4091],\n",
       "         [-0.2901, -0.3117,  0.2737, -0.0633, -0.1364,  0.3015,  0.2795, -0.1159],\n",
       "         [-0.2506, -0.1455,  0.0848, -0.3716,  0.2604,  0.1226, -0.3731,  0.0875],\n",
       "         [ 0.0419, -0.3324, -0.2248, -0.1150,  0.2328, -0.3329, -0.3717,  0.1777],\n",
       "         [ 0.0973,  0.0194, -0.4303,  0.2819, -0.2296,  0.0691, -0.2529, -0.1779],\n",
       "         [-0.1394, -0.3209,  0.0006,  0.1159,  0.0797,  0.0980, -0.0742,  0.4225],\n",
       "         [-0.1583, -0.0803,  0.1127, -0.4108,  0.1142, -0.1683, -0.2880, -0.3119],\n",
       "         [ 0.2231, -0.0517, -0.0994,  0.3291,  0.2447,  0.3129, -0.1433, -0.4309],\n",
       "         [-0.0900, -0.3122,  0.0093, -0.0194, -0.0431,  0.2455,  0.4196,  0.4299],\n",
       "         [ 0.0191, -0.3231, -0.2746, -0.3516,  0.4164,  0.1371, -0.0944, -0.2250],\n",
       "         [ 0.0884, -0.3381, -0.1014, -0.4073, -0.4085, -0.2288,  0.1324, -0.1296]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.0621,  0.0387,  0.0901,  0.0911, -0.0931, -0.2900,  0.2382,  0.0742],\n",
       "         [-0.2011, -0.1973, -0.0463,  0.2109,  0.3368,  0.3349,  0.2446, -0.3127],\n",
       "         [-0.1423, -0.1978,  0.2255, -0.1286, -0.1253, -0.2212, -0.2313,  0.3399],\n",
       "         [ 0.3243, -0.3361, -0.0460,  0.2717, -0.2599, -0.2790,  0.0529,  0.3437],\n",
       "         [-0.3074,  0.3519, -0.1036, -0.1724,  0.1412,  0.1503, -0.0345,  0.0509],\n",
       "         [ 0.0987, -0.1255, -0.1969,  0.2533,  0.3011, -0.1251,  0.3102,  0.1404],\n",
       "         [-0.3190,  0.3504,  0.0233, -0.0723, -0.0668,  0.2666,  0.2914, -0.2476],\n",
       "         [-0.2535, -0.2836,  0.1747,  0.3373,  0.2550, -0.2156,  0.1705, -0.1072]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(attention.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
